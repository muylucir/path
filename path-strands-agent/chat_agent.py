"""
Chat Agent for PATH Step 2 - ëŒ€í™”í˜• ë¶„ì„ (ìŠ¤íŠ¸ë¦¬ë° + ì±„íŒ… ì§€ì›)

ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ê³  í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•˜ì—¬ Feasibilityë¥¼ í‰ê°€í•˜ëŠ” Agent
"""

from strands import Agent
from typing import Dict, List, Any, AsyncIterator
import json
import os
import re
from safe_tools import safe_file_read
from strands_utils import strands_utils, get_skill_prompt
from token_tracker import extract_usage
from prompts import (
    SYSTEM_PROMPT,
    get_initial_analysis_prompt,
    FEASIBILITY_SYSTEM_PROMPT,
    get_feasibility_evaluation_prompt,
    get_feasibility_reevaluation_prompt,
    PATTERN_ANALYSIS_SYSTEM_PROMPT,
    get_pattern_analysis_prompt,
    get_pattern_chat_prompt,
    get_pattern_finalize_prompt,
)

# Default model ID - can be overridden via environment variable
DEFAULT_MODEL_ID = os.environ.get("BEDROCK_MODEL_ID", "global.anthropic.claude-opus-4-6-v1")

def _extract_json(response_text: str, context: str = "response") -> Dict[str, Any]:
    """LLM ì‘ë‹µì—ì„œ JSONì„ ì¶”ì¶œí•˜ê³  íŒŒì‹±.

    Args:
        response_text: LLM ì‘ë‹µ ì „ì²´ í…ìŠ¤íŠ¸
        context: ì—ëŸ¬ ë©”ì‹œì§€ì— í¬í•¨í•  ì»¨í…ìŠ¤íŠ¸ ì„¤ëª…

    Returns:
        íŒŒì‹±ëœ JSON dict

    Raises:
        ValueError: JSONì„ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” ê²½ìš°
        json.JSONDecodeError: JSON íŒŒì‹±ì— ì‹¤íŒ¨í•œ ê²½ìš°
    """
    # ```json ... ``` ë¸”ë¡ ë¨¼ì € ì‹œë„
    json_block = re.search(r'''```json\s*\n(.*?)\n\s*```''', response_text, re.DOTALL)
    if json_block:
        return json.loads(json_block.group(1))

    # { ... } ì¶”ì¶œ
    json_start = response_text.find("{")
    json_end = response_text.rfind("}") + 1

    if json_start != -1 and json_end > json_start:
        json_str = response_text[json_start:json_end]
        return json.loads(json_str)

    raise ValueError(f"Failed to extract JSON from {context}")


# LEGACY: /analyze ì—”ë“œí¬ì¸íŠ¸ ì „ìš© â€” ìƒˆ í”Œë¡œìš°ì—ì„œëŠ” FeasibilityAgent ì‚¬ìš©
class AnalyzerAgent:
    """ì‚¬ìš©ì ì…ë ¥(pain point, input, process, output ë“±)ì„ ë¶„ì„í•˜ëŠ” Agent"""

    def __init__(self, model_id: str = DEFAULT_MODEL_ID):
        self.agent = Agent(
            model=model_id,
            system_prompt=SYSTEM_PROMPT,
            callback_handler=None  # ì½˜ì†” ì¶œë ¥ ë¹„í™œì„±í™”
        )
    
    def analyze(self, form_data: Dict[str, Any]) -> str:
        """ì´ˆê¸° ë¶„ì„ ìˆ˜í–‰ - ë™ê¸° ë²„ì „"""
        prompt = get_initial_analysis_prompt(form_data)
        result = self.agent(prompt)
        return result.message['content'][0]['text']
    
    async def analyze_stream(self, form_data: Dict[str, Any]) -> AsyncIterator[str]:
        """ì´ˆê¸° ë¶„ì„ ìˆ˜í–‰ - ìŠ¤íŠ¸ë¦¬ë° ë²„ì „"""
        prompt = get_initial_analysis_prompt(form_data)
        
        async for event in self.agent.stream_async(prompt):
            if "data" in event:
                yield event["data"]


# LEGACY: /chat ì—”ë“œí¬ì¸íŠ¸ ì „ìš© â€” ìƒˆ í”Œë¡œìš°ì—ì„œëŠ” PatternAnalyzerAgent ì‚¬ìš©
class ChatAgent:
    """ëŒ€í™”í˜• ë¶„ì„ Agent - ì±„íŒ… ì§€ì›"""

    def __init__(self, model_id: str = DEFAULT_MODEL_ID):
        self.agent = Agent(
            model=model_id,
            system_prompt=SYSTEM_PROMPT,
            callback_handler=None  # ì½˜ì†” ì¶œë ¥ ë¹„í™œì„±í™”
        )
        self.conversation_history: List[Dict[str, str]] = []
    
    def add_message(self, role: str, content: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€"""
        self.conversation_history.append({"role": role, "content": content})
    
    def get_history(self) -> List[Dict[str, str]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.conversation_history
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
    
    def chat(self, user_message: str) -> str:
        """ì±„íŒ… - ë™ê¸° ë²„ì „"""
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.add_message("user", user_message)
        
        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        history_text = "\n\n".join([
            f"{msg['role'].upper()}: {msg['content']}" 
            for msg in self.conversation_history
        ])
        
        prompt = f"""{history_text}

ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë°˜ì˜í•˜ì—¬:
1. ì¶”ê°€ ì •ë³´ê°€ ë” í•„ìš”í•˜ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸ (ìµœëŒ€ 3ê°œ)
2. ì¶©ë¶„í•˜ë©´ "ì´ì œ ìµœì¢… ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'ë¶„ì„ ì™„ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”." ì•ˆë‚´

ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”."""
        
        result = self.agent(prompt)
        response = result.message['content'][0]['text']
        
        # ì‘ë‹µ ì¶”ê°€
        self.add_message("assistant", response)
        
        return response
    
    async def chat_stream(self, user_message: str) -> AsyncIterator[str]:
        """ì±„íŒ… - ìŠ¤íŠ¸ë¦¬ë° ë²„ì „"""
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.add_message("user", user_message)
        
        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        history_text = "\n\n".join([
            f"{msg['role'].upper()}: {msg['content']}" 
            for msg in self.conversation_history
        ])
        
        prompt = f"""{history_text}

ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë°˜ì˜í•˜ì—¬:
1. ì¶”ê°€ ì •ë³´ê°€ ë” í•„ìš”í•˜ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸ (ìµœëŒ€ 3ê°œ)
2. ì¶©ë¶„í•˜ë©´ "ì´ì œ ìµœì¢… ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'ë¶„ì„ ì™„ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”." ì•ˆë‚´

ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”."""
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìˆ˜ì§‘
        full_response = ""
        async for event in self.agent.stream_async(prompt):
            if "data" in event:
                chunk = event["data"]
                full_response += chunk
                yield chunk
        
        # ì „ì²´ ì‘ë‹µ ì¶”ê°€
        self.add_message("assistant", full_response)


# LEGACY: /finalize ì—”ë“œí¬ì¸íŠ¸ ì „ìš© â€” ìƒˆ í”Œë¡œìš°ì—ì„œëŠ” PatternAnalyzerAgent.finalize() ì‚¬ìš©
class EvaluatorAgent:
    """ë‹µë³€ ìˆ˜ì§‘ í›„ Feasibility ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” Agent"""

    def __init__(self, model_id: str = DEFAULT_MODEL_ID):
        self.agent = Agent(
            model=model_id,
            system_prompt=SYSTEM_PROMPT,
            callback_handler=None  # ì½˜ì†” ì¶œë ¥ ë¹„í™œì„±í™”
        )
    
    def evaluate(self, form_data: Dict[str, Any], conversation: List[Dict]) -> Dict[str, Any]:
        """Feasibility í‰ê°€ ìˆ˜í–‰ - PATH ì›¹ì•± í˜•ì‹"""
        conversation_text = "\n".join([
            f"{msg['role'].upper()}: {msg['content']}" 
            for msg in conversation
        ])
        
        prompt = f"""ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ì˜ ë¶„ì„ ë‚´ìš©ì…ë‹ˆë‹¤:

{conversation_text}

ì´ì œ ìµœì¢… ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”. ë‹¤ìŒì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:

**Universal Agent Design Patterns**:
- **ReAct**: ë‹¨ê³„ì  ì¶”ë¡ (Think) â†’ ë„êµ¬ ì‚¬ìš©(Act) â†’ ê²°ê³¼ ê´€ì°°(Observe) â†’ ë°˜ë³µ
- **Reflection**: ì¶œë ¥ ìƒì„± â†’ í’ˆì§ˆ ê²€í†  â†’ ê°œì„  ë°˜ë³µ (ìê¸° ì„±ì°° ë£¨í”„)
- **Tool Use**: ì™¸ë¶€ ë„êµ¬/APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ì ‘ê·¼, ê³„ì‚°, ì‹œìŠ¤í…œ ì—°ë™
- **Planning**: ë³µì¡í•œ ì‘ì—…ì„ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ìˆœì°¨ ì‹¤í–‰
- **Multi-Agent**: ì „ë¬¸í™”ëœ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ ì—­í•  ë¶„ë‹´í•˜ì—¬ í˜‘ì—…
- **Human-in-the-Loop**: Agent ì œì•ˆ â†’ ì‚¬ëŒ ê²€í† /ìŠ¹ì¸ â†’ ì‹¤í–‰

**íŒ¨í„´ ì¡°í•©ë„ ê°€ëŠ¥**: ì˜ˆ) "ReAct + Tool Use", "Planning + Multi-Agent"

{{
  "pain_point": "{form_data.get('painPoint', '')}",
  "input_type": "INPUT íƒ€ì…",
  "input_detail": "INPUT ìƒì„¸",
  "process_steps": ["ë‹¨ê³„1: ì„¤ëª…", "ë‹¨ê³„2: ì„¤ëª…", "..."],
  "output_types": ["OUTPUT íƒ€ì…1", "OUTPUT íƒ€ì…2"],
  "output_detail": "OUTPUT ìƒì„¸",
  "human_loop": "None/Review/Exception/Collaborate",
  "pattern": "ReAct/Reflection/Tool Use/Planning/Multi-Agent/Human-in-the-Loop (ì¡°í•© ê°€ëŠ¥)",
  "pattern_reason": "íŒ¨í„´ ì„ íƒ ì´ìœ  (ë¬¸ì œì˜ íŠ¹ì„±ê³¼ íŒ¨í„´ì˜ ì í•©ì„± ì„¤ëª…)",
  "feasibility_breakdown": {{
    "data_access": 0-10,
    "decision_clarity": 0-10,
    "error_tolerance": 0-10,
    "latency": 0-10,
    "integration": 0-10
  }},
  "feasibility_score": 0-50,
  "recommendation": "ì¶”ì²œ ì‚¬í•­",
  "risks": ["ë¦¬ìŠ¤í¬1", "ë¦¬ìŠ¤í¬2"],
  "next_steps": [
    "Phase 1: í•µì‹¬ ê¸°ëŠ¥ í”„ë¡œí† íƒ€ì… - ì„¤ëª…",
    "Phase 2: ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ - ì„¤ëª…",
    "Phase 3: (ì„ íƒì ) ê°œì„  ë° í™•ì¥ - ì„¤ëª…"
  ]
}}

ì¤‘ìš”:
- pain_pointëŠ” ìœ„ì— ì§€ì •ëœ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. ìš”ì•½í•˜ê±°ë‚˜ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”.
- next_stepsëŠ” ì£¼ ë‹¨ìœ„ ê¸°ê°„ì´ ì•„ë‹Œ Phase/ë‹¨ê³„ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
        
        system_prompt_for_json = f"""{SYSTEM_PROMPT}

ë‹¹ì‹ ì€ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."""
        
        # Agent ì¬ìƒì„± (JSON ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)
        json_agent = Agent(
            model=self.agent.model.config['model_id'],
            system_prompt=system_prompt_for_json,
            callback_handler=None  # ì½˜ì†” ì¶œë ¥ ë¹„í™œì„±í™”
        )
        
        result = json_agent(prompt)
        response_text = result.message['content'][0]['text']

        return _extract_json(response_text, "evaluation")


class FeasibilityAgent:
    """Step2: Feasibility í‰ê°€ ì „ìš© Agent"""

    def __init__(self, model_id: str = DEFAULT_MODEL_ID):
        # Skill ì‹œìŠ¤í…œ ì´ˆê¸°í™” (cached)
        skill_prompt = get_skill_prompt()
        enhanced_prompt = FEASIBILITY_SYSTEM_PROMPT + "\n" + skill_prompt

        self.agent = strands_utils.get_agent(
            system_prompts=enhanced_prompt,
            model_id=model_id,
            max_tokens=8192,
            temperature=0.3,
            tools=[safe_file_read]
        )

    def evaluate(self, form_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì´ˆê¸° Feasibility í‰ê°€ ìˆ˜í–‰"""
        prompt = get_feasibility_evaluation_prompt(form_data)
        result = self.agent(prompt)
        response_text = result.message['content'][0]['text']
        parsed = _extract_json(response_text, "feasibility evaluation")
        parsed["_usage"] = extract_usage(result)
        return parsed

    async def evaluate_stream(self, form_data: Dict[str, Any]) -> AsyncIterator[str]:
        """ì´ˆê¸° Feasibility í‰ê°€ ìˆ˜í–‰ - SSE ìŠ¤íŠ¸ë¦¬ë° (Progress í¬í•¨)"""
        import asyncio

        prompt = get_feasibility_evaluation_prompt(form_data)

        # í‰ê°€ í•­ëª© ë‹¨ê³„
        stages = [
            "ë°ì´í„° ì ‘ê·¼ì„± ë¶„ì„ ì¤‘...",
            "íŒë‹¨ ëª…í™•ì„± ë¶„ì„ ì¤‘...",
            "ì˜¤ë¥˜ í—ˆìš©ë„ ë¶„ì„ ì¤‘...",
            "ì‘ë‹µì†ë„ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì¤‘...",
            "ì‹œìŠ¤í…œ ì—°ë™ ë¶„ì„ ì¤‘...",
        ]

        # ì‹œì‘ ì•Œë¦¼
        yield json.dumps({"stage": "ì¤€ë¹„ë„ ì ê²€ ì‹œì‘", "progress": 0}, ensure_ascii=False)

        # LLM í˜¸ì¶œì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
        task = asyncio.create_task(asyncio.to_thread(self._evaluate_sync, prompt))

        # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ (3ì´ˆë§ˆë‹¤)
        progress = 10
        stage_idx = 0
        while not task.done():
            await asyncio.sleep(3)
            if not task.done():
                progress = min(progress + 15, 85)
                stage = stages[stage_idx % len(stages)]
                stage_idx += 1
                yield json.dumps({"stage": stage, "progress": progress}, ensure_ascii=False)

        # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        result = await task
        usage = result.pop("_usage", None)

        # ì™„ë£Œ ë° ê²°ê³¼ ì „ì†¡
        yield json.dumps({"stage": "ë¶„ì„ ì™„ë£Œ", "progress": 100}, ensure_ascii=False)
        yield json.dumps({"result": result}, ensure_ascii=False)
        if usage:
            yield json.dumps({"usage": usage}, ensure_ascii=False)

    def _evaluate_sync(self, prompt: str) -> Dict[str, Any]:
        """ë™ê¸° í‰ê°€ (ë‚´ë¶€ìš©)"""
        result = self.agent(prompt)
        response_text = result.message['content'][0]['text']
        parsed = _extract_json(response_text, "feasibility evaluation")
        parsed["_usage"] = extract_usage(result)
        return parsed

    def reevaluate(self, form_data: Dict[str, Any], previous_evaluation: Dict[str, Any], improvement_plans: Dict[str, str]) -> Dict[str, Any]:
        """ê°œì„ ì•ˆ ë°˜ì˜ ì¬í‰ê°€ ìˆ˜í–‰"""
        prompt = get_feasibility_reevaluation_prompt(form_data, previous_evaluation, improvement_plans)
        result = self.agent(prompt)
        response_text = result.message['content'][0]['text']
        parsed = _extract_json(response_text, "feasibility re-evaluation")
        parsed["_usage"] = extract_usage(result)
        return parsed


class PatternAnalyzerAgent:
    """Step3: Feasibility ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒ¨í„´ ë¶„ì„í•˜ëŠ” Agent (Skill ì‹œìŠ¤í…œ ì§€ì›)"""

    def __init__(self, model_id: str = DEFAULT_MODEL_ID):
        # Skill ì‹œìŠ¤í…œ ì´ˆê¸°í™” (cached)
        skill_prompt = get_skill_prompt()
        enhanced_prompt = PATTERN_ANALYSIS_SYSTEM_PROMPT + "\n" + skill_prompt

        self.agent = strands_utils.get_agent(
            system_prompts=enhanced_prompt,
            model_id=model_id,
            max_tokens=16000,
            temperature=0.3,
            tools=[safe_file_read]
        )
        # Stateful ëª¨ë“œì—ì„œ ì¶©ë¶„í•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ (ê¸°ë³¸ 40 â†’ 200)
        self.agent.conversation_manager.window_size = 200
        self.conversation_history: List[Dict[str, str]] = []

    def add_message(self, role: str, content: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€"""
        self.conversation_history.append({"role": role, "content": content})

    def get_history(self) -> List[Dict[str, str]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.conversation_history

    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []

    def analyze(self, form_data: Dict[str, Any], feasibility: Dict[str, Any]) -> str:
        """Feasibility ê¸°ë°˜ ì´ˆê¸° íŒ¨í„´ ë¶„ì„ - ë™ê¸° ë²„ì „"""
        prompt = get_pattern_analysis_prompt(form_data, feasibility)
        result = self.agent(prompt)
        response = result.message['content'][0]['text']
        self.add_message("assistant", response)
        return response

    async def analyze_stream(self, form_data: Dict[str, Any], feasibility: Dict[str, Any], improvement_plans: Dict[str, str] = None) -> AsyncIterator[dict]:
        """Feasibility ê¸°ë°˜ ì´ˆê¸° íŒ¨í„´ ë¶„ì„ - ìŠ¤íŠ¸ë¦¬ë° ë²„ì „ (dict yield)"""
        prompt = get_pattern_analysis_prompt(form_data, feasibility, improvement_plans)

        full_response = ""
        usage = None
        async for event in self.agent.stream_async(prompt):
            if "data" in event:
                chunk = event["data"]
                full_response += chunk
                yield {"text": chunk}
            elif "result" in event:
                usage = extract_usage(event["result"])

        self.add_message("assistant", full_response)
        if usage:
            yield {"usage": usage}

    async def chat_stream(self, user_message: str, stateful: bool = False) -> AsyncIterator[dict]:
        """íŒ¨í„´ ê´€ë ¨ ëŒ€í™” - ìŠ¤íŠ¸ë¦¬ë° ë²„ì „ (Skill ì‹œìŠ¤í…œ ì§€ì›, dict yield)"""
        self.add_message("user", user_message)

        if stateful:
            prompt = get_pattern_chat_prompt(user_message=user_message)
        else:
            history_text = "\n\n".join([
                f"{msg['role'].upper()}: {msg['content']}"
                for msg in self.conversation_history
            ])
            prompt = get_pattern_chat_prompt(user_message=user_message, history_text=history_text)

        full_response = ""
        usage = None
        async for event in self.agent.stream_async(prompt):
            if "data" in event:
                chunk = event["data"]
                full_response += chunk
                yield {"text": chunk}
            elif "result" in event:
                usage = extract_usage(event["result"])

        self.add_message("assistant", full_response)
        if usage:
            yield {"usage": usage}

    def finalize(self, form_data: Dict[str, Any], feasibility: Dict[str, Any], improvement_plans: Dict[str, str] = None, stateful: bool = False) -> Dict[str, Any]:
        """íŒ¨í„´ í™•ì • ë° ìµœì¢… ë¶„ì„ ê²°ê³¼ ìƒì„± (ê°œì„ ëœ ì ìˆ˜ í¬í•¨)"""
        if stateful:
            conversation_text = None
        else:
            conversation_text = "\n".join([
                f"{msg['role'].upper()}: {msg['content']}"
                for msg in self.conversation_history
            ])

        prompt = get_pattern_finalize_prompt(
            form_data, feasibility,
            improvement_plans=improvement_plans,
            conversation_text=conversation_text
        )

        result = self.agent(prompt)
        response_text = result.message['content'][0]['text']
        parsed = _extract_json(response_text, "pattern finalization")

        # improved_feasibility ìœ íš¨ì„± ê²€ì¦: ë¶ˆì™„ì „í•œ ê°ì²´ ë°©ì–´
        improved = parsed.get("improved_feasibility")
        if improved is not None:
            if (not isinstance(improved, dict)
                or not isinstance(improved.get("score"), (int, float))
                or not isinstance(improved.get("score_change"), (int, float))):
                parsed["improved_feasibility"] = None

        parsed["_usage"] = extract_usage(result)
        return parsed


# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    import asyncio

    async def test():
        # ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
        print("ğŸ” ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ í…ŒìŠ¤íŠ¸")
        print("="*80)
        analyzer = AnalyzerAgent()
        async for chunk in analyzer.analyze_stream(form_data):
            print(chunk, end="", flush=True)
        print("\n\nâœ… ì™„ë£Œ!")

    asyncio.run(test())

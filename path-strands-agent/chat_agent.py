"""
Chat Agent for PATH Step 2 - ëŒ€í™”í˜• ë¶„ì„ (ìŠ¤íŠ¸ë¦¬ë° + ì±„íŒ… ì§€ì›)

ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ê³  í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•˜ì—¬ Feasibilityë¥¼ í‰ê°€í•˜ëŠ” Agent
"""

from strands import Agent
from typing import Dict, List, Any, AsyncIterator
import json
import re
from prompts import SYSTEM_PROMPT, get_initial_analysis_prompt


class AnalyzerAgent:
    """ì‚¬ìš©ì ì…ë ¥(pain point, input, process, output ë“±)ì„ ë¶„ì„í•˜ëŠ” Agent"""
    
    def __init__(self, model_id: str = "global.anthropic.claude-sonnet-4-5-20250929-v1:0"):
        self.agent = Agent(
            model=model_id,
            system_prompt=SYSTEM_PROMPT
        )
    
    def analyze(self, form_data: Dict[str, Any]) -> str:
        """ì´ˆê¸° ë¶„ì„ ìˆ˜í–‰ - ë™ê¸° ë²„ì „"""
        prompt = get_initial_analysis_prompt(form_data)
        result = self.agent(prompt)
        return result.message['content'][0]['text']
    
    async def analyze_stream(self, form_data: Dict[str, Any]) -> AsyncIterator[str]:
        """ì´ˆê¸° ë¶„ì„ ìˆ˜í–‰ - ìŠ¤íŠ¸ë¦¬ë° ë²„ì „"""
        prompt = get_initial_analysis_prompt(form_data)
        
        async for event in self.agent.stream_async(prompt):
            if "data" in event:
                yield event["data"]


class ChatAgent:
    """ëŒ€í™”í˜• ë¶„ì„ Agent - ì±„íŒ… ì§€ì›"""
    
    def __init__(self, model_id: str = "global.anthropic.claude-sonnet-4-5-20250929-v1:0"):
        self.agent = Agent(
            model=model_id,
            system_prompt=SYSTEM_PROMPT
        )
        self.conversation_history: List[Dict[str, str]] = []
    
    def add_message(self, role: str, content: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€"""
        self.conversation_history.append({"role": role, "content": content})
    
    def get_history(self) -> List[Dict[str, str]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.conversation_history
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
    
    def chat(self, user_message: str) -> str:
        """ì±„íŒ… - ë™ê¸° ë²„ì „"""
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.add_message("user", user_message)
        
        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        history_text = "\n\n".join([
            f"{msg['role'].upper()}: {msg['content']}" 
            for msg in self.conversation_history
        ])
        
        prompt = f"""{history_text}

ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë°˜ì˜í•˜ì—¬:
1. ì¶”ê°€ ì •ë³´ê°€ ë” í•„ìš”í•˜ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸ (ìµœëŒ€ 3ê°œ)
2. ì¶©ë¶„í•˜ë©´ "ì´ì œ ìµœì¢… ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'ë¶„ì„ ì™„ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”." ì•ˆë‚´

ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”."""
        
        result = self.agent(prompt)
        response = result.message['content'][0]['text']
        
        # ì‘ë‹µ ì¶”ê°€
        self.add_message("assistant", response)
        
        return response
    
    async def chat_stream(self, user_message: str) -> AsyncIterator[str]:
        """ì±„íŒ… - ìŠ¤íŠ¸ë¦¬ë° ë²„ì „"""
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.add_message("user", user_message)
        
        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        history_text = "\n\n".join([
            f"{msg['role'].upper()}: {msg['content']}" 
            for msg in self.conversation_history
        ])
        
        prompt = f"""{history_text}

ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë°˜ì˜í•˜ì—¬:
1. ì¶”ê°€ ì •ë³´ê°€ ë” í•„ìš”í•˜ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸ (ìµœëŒ€ 3ê°œ)
2. ì¶©ë¶„í•˜ë©´ "ì´ì œ ìµœì¢… ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'ë¶„ì„ ì™„ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”." ì•ˆë‚´

ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”."""
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìˆ˜ì§‘
        full_response = ""
        async for event in self.agent.stream_async(prompt):
            if "data" in event:
                chunk = event["data"]
                full_response += chunk
                yield chunk
        
        # ì „ì²´ ì‘ë‹µ ì¶”ê°€
        self.add_message("assistant", full_response)


class EvaluatorAgent:
    """ë‹µë³€ ìˆ˜ì§‘ í›„ Feasibility ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” Agent"""
    
    def __init__(self, model_id: str = "global.anthropic.claude-sonnet-4-5-20250929-v1:0"):
        self.agent = Agent(
            model=model_id,
            system_prompt=SYSTEM_PROMPT
        )
    
    def evaluate(self, form_data: Dict[str, Any], conversation: List[Dict]) -> Dict[str, Any]:
        """Feasibility í‰ê°€ ìˆ˜í–‰ - PATH ì›¹ì•± í˜•ì‹"""
        conversation_text = "\n".join([
            f"{msg['role'].upper()}: {msg['content']}" 
            for msg in conversation
        ])
        
        prompt = f"""ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ì˜ ë¶„ì„ ë‚´ìš©ì…ë‹ˆë‹¤:

{conversation_text}

ì´ì œ ìµœì¢… ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”. ë‹¤ìŒì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:

**íŒ¨í„´ ìš©ì–´ ì •ì˜ (í˜¼ë™ ì£¼ì˜)**:
- **Tool Use**: Agentê°€ MCP ì„œë²„, ì™¸ë¶€ API, ë°ì´í„°ë² ì´ìŠ¤ ë“± ì™¸ë¶€ ë„êµ¬ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” íŒ¨í„´
- **Agent-as-Tool**: ë‹¤ë¥¸ Agentê°€ íŠ¹ì • Agentë¥¼ "ë„êµ¬"ë¡œ ì‚¬ìš©í•˜ëŠ” íŒ¨í„´ (Agent â†’ Agent í˜¸ì¶œ)
- **Planning**: ë³µì¡í•œ ì‘ì—…ì„ ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•˜ì—¬ ìˆœì°¨ ì‹¤í–‰
- **Reflection**: ì¶œë ¥ í’ˆì§ˆ ê²€ì¦ í›„ ìê°€ ê°œì„  ë£¨í”„
- **Multi-Agent (Graph)**: ì—¬ëŸ¬ Agentê°€ í˜‘ì—…í•˜ëŠ” êµ¬ì¡°

**ì¤‘ìš”**: pattern_reason ì‘ì„± ì‹œ "Tool Use"ì™€ "Agent-as-Tool"ì„ í˜¼ë™í•˜ì§€ ë§ˆì„¸ìš”.
- MCP/API í˜¸ì¶œ = Tool Use (Agent-as-Tool ì•„ë‹˜)
- Agentê°€ ë‹¤ë¥¸ Agentë¥¼ ë„êµ¬ë¡œ ì‚¬ìš© = Agent-as-Tool

{{
  "pain_point": "ì‚¬ìš©ì Pain Point",
  "input_type": "INPUT íƒ€ì…",
  "input_detail": "INPUT ìƒì„¸",
  "process_steps": ["ë‹¨ê³„1: ì„¤ëª…", "ë‹¨ê³„2: ì„¤ëª…", "..."],
  "output_types": ["OUTPUT íƒ€ì…1", "OUTPUT íƒ€ì…2"],
  "output_detail": "OUTPUT ìƒì„¸",
  "human_loop": "None/Review/Exception/Collaborate",
  "pattern": "Reflection/Tool Use/Planning/Multi-Agent",
  "pattern_reason": "íŒ¨í„´ ì„ íƒ ì´ìœ  (Tool Useì™€ Agent-as-Tool êµ¬ë¶„í•˜ì—¬ ì •í™•íˆ ê¸°ìˆ )",
  "feasibility_breakdown": {{
    "data_access": 0-10,
    "decision_clarity": 0-10,
    "error_tolerance": 0-10,
    "latency": 0-10,
    "integration": 0-10
  }},
  "feasibility_score": 0-50,
  "recommendation": "ì¶”ì²œ ì‚¬í•­",
  "risks": ["ë¦¬ìŠ¤í¬1", "ë¦¬ìŠ¤í¬2"],
  "next_steps": [
    "Phase 1: í•µì‹¬ ê¸°ëŠ¥ í”„ë¡œí† íƒ€ì… - ì„¤ëª…",
    "Phase 2: ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ - ì„¤ëª…",
    "Phase 3: (ì„ íƒì ) ê°œì„  ë° í™•ì¥ - ì„¤ëª…"
  ]
}}

ì¤‘ìš”: next_stepsëŠ” ì£¼ ë‹¨ìœ„ ê¸°ê°„ì´ ì•„ë‹Œ Phase/ë‹¨ê³„ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
        
        system_prompt_for_json = f"""{SYSTEM_PROMPT}

ë‹¹ì‹ ì€ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."""
        
        # Agent ì¬ìƒì„± (JSON ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)
        json_agent = Agent(
            model=self.agent.model.config['model_id'],
            system_prompt=system_prompt_for_json
        )
        
        result = json_agent(prompt)
        response_text = result.message['content'][0]['text']
        
        # JSON ì¶”ì¶œ
        json_start = response_text.find("{")
        json_end = response_text.rfind("}") + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = response_text[json_start:json_end]
            return json.loads(json_str)
        else:
            raise ValueError("Failed to extract JSON from evaluation response")


# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    import asyncio
    
    async def test():
        # ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
        print("ğŸ” ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ í…ŒìŠ¤íŠ¸")
        print("="*80)
        analyzer = AnalyzerAgent()
        async for chunk in analyzer.analyze_stream(form_data):
            print(chunk, end="", flush=True)
        print("\n\nâœ… ì™„ë£Œ!")
    
    asyncio.run(test())
